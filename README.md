# ğŸ•·ï¸ ScrappyDoo - Scraper Web Avanzado

## ğŸ“‹ DescripciÃ³n
ScrappyDoo es un scraper web potente y flexible construido en Node.js, diseÃ±ado para extraer datos de sitios web dinÃ¡micos con caracterÃ­sticas avanzadas anti-detecciÃ³n y alto rendimiento.

## â­ CaracterÃ­sticas Principales
- ğŸ”„ RotaciÃ³n automÃ¡tica de proxies
- ğŸ‘¤ RotaciÃ³n de User-Agents
- ğŸ¤– ResoluciÃ³n automÃ¡tica de CAPTCHA
- ğŸ“Š Sistema de colas para scraping masivo
- ğŸ›¡ï¸ Mecanismos anti-bloqueo
- ğŸ“¦ Almacenamiento en MongoDB
- ğŸ“ Logging detallado

## ğŸš€ InstalaciÃ³n

### Prerrequisitos
- Node.js >= 20.0.0
- MongoDB
- Redis

### Pasos de InstalaciÃ³n

```bash
# Clonar el repositorio
git clone https://github.com/tuusuario/scrappy-doo.git

# Entrar al directorio
cd scrappy-doo

# Instalar dependencias
npm install

# Instalar navegador de Playwright
npx playwright install chromium
```

### ConfiguraciÃ³n
1. Copia el archivo `.env.example` a `.env`:
```bash
cp .env.example .env
```

2. Configura las variables de entorno en el archivo `.env`:
```plaintext
REDIS_HOST=127.0.0.1
REDIS_PORT=6379
MONGODB_URI=mongodb://127.0.0.1:27017/scrappy-doo
CAPTCHA_API_KEY=tu_api_key_2captcha

# Configuraciones de seguridad
NODE_ENV=production
RATE_LIMIT=100
CONCURRENT_SCRAPES=5
RETRY_ATTEMPTS=3
DELAY_BETWEEN_REQUESTS=2000
```

## ğŸš€ Uso

```bash
# Iniciar el scraper
npm start

# Modo desarrollo
npm run dev
```

## ğŸ“¦ Estructura del Proyecto
```
scrappy-doo/
â”œâ”€â”€ config/                  # Archivos de configuraciÃ³n
â”‚   â”œâ”€â”€ proxies.json        # Lista de proxies
â”‚   â””â”€â”€ user-agents.json    # User agents
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/               # Funcionalidad principal
â”‚   â”œâ”€â”€ scrapers/           # Scrapers especÃ­ficos
â”‚   â”œâ”€â”€ utils/              # Utilidades
â”‚   â””â”€â”€ database/           # Capa de datos
â”œâ”€â”€ .env                    # Variables de entorno
â””â”€â”€ package.json
```

## ğŸ› ï¸ TecnologÃ­as Utilizadas
- Node.js
- Playwright
- MongoDB
- Redis
- Bull (sistema de colas)
- Winston (logging)

- `POST /api/scrape`: Encolar nuevos trabajos de scraping
- `GET /api/jobs/:jobId`: Obtener estado de un trabajo
- `GET /api/jobs`: Listar todos los trabajos
- `POST /api/jobs/:jobId/retry`: Reintentar trabajo fallido
- `DELETE /api/jobs/:jobId`: Cancelar trabajo

### Ajustes de Rate Limiting
Modifica las siguientes variables en `.env`:
- `RATE_LIMIT`: Peticiones por minuto
- `CONCURRENT_SCRAPES`: Scrapes simultÃ¡neos
- `DELAY_BETWEEN_REQUESTS`: Delay entre peticiones (ms)

### ConfiguraciÃ³n de Proxies
- AÃ±ade mÃ¡s proxies en `config/proxies.json`
- El sistema rotarÃ¡ automÃ¡ticamente entre ellos
- Los proxies bloqueados se marcan automÃ¡ticamente

## ğŸ¤ Contribuir
Las contribuciones son bienvenidas. Por favor, lee `CONTRIBUTING.md` para detalles sobre nuestro cÃ³digo de conducta y el proceso para enviarnos pull requests.

## ğŸ“„ Licencia
Este proyecto estÃ¡ bajo la Licencia MIT - ver el archivo `LICENSE` para mÃ¡s detalles.

## âš ï¸ Aviso Legal
Este scraper debe usarse de manera Ã©tica y legal, respetando los tÃ©rminos de servicio de los sitios web y las leyes de protecciÃ³n de datos aplicables.

## ğŸ”Œ IntegraciÃ³n con n8n

### ConfiguraciÃ³n
1. Configura las variables de n8n en `.env`:
```plaintext
N8N_WEBHOOK_URL=http://localhost:5678/webhook/scrappy-doo
N8N_API_KEY=tu_n8n_api_key
N8N_EVENTS_WEBHOOK=http://localhost:5678/webhook/scrappy-events
```

### Eventos Disponibles
- `job_created`: Cuando se crea un nuevo trabajo
- `job_completed`: Cuando un trabajo se completa exitosamente
- `job_failed`: Cuando un trabajo falla
- `job_stalled`: Cuando un trabajo se estanca

### Ejemplo de Workflow
1. Crear un nodo "Webhook" en n8n
2. Configurar la URL y la API key
3. Procesar los datos recibidos segÃºn necesidad
